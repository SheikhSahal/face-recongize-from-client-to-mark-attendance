import face_recognition
import numpy as np
from datetime import datetime
import os
import cv2
from flask import Flask, render_template, Response, request, redirect, url_for
import csv
import dlib

app = Flask(__name__)

# Global variables
path = "ImagesAttendance"
only_name = "only_name"
attend_csv_path = "Attendance.csv"
images = []
image_names = []
encodeListKnown = []
attend_dict = {}

# Initialize the dlib face detector and shape predictor
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")

# Clean known images folder
def clean():
    for f in os.listdir(only_name):
        os.remove(os.path.join(only_name, f))

# Access images in folder
def access():
    global images, image_names
    mylist = os.listdir(path)
    for cl in mylist:
        curImg = cv2.imread(f'{path}/{cl}')
        images.append(curImg)
        image_names.append(os.path.splitext(cl)[0])
    print(image_names)

# Return the 128-dimension face encoding for each face in the image
def find_encodings(images):
    encodeList = []
    for image in images:
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        encode = face_recognition.face_encodings(image)
        if encode:
            encodeList.append(encode[0])
    return encodeList

# Save the captured image
def save_img(imagesz, nami):
    savedImg = os.listdir(only_name)
    if nami not in savedImg:
        cv2.imwrite(os.path.join(only_name, f"{nami}.jpg"), imagesz)

# Mark attendance in CSV file
def markAttendance(name):
    print(name, "attended")
    now = datetime.now()
    dtString = now.strftime("%I:%M %p")
    
    if name not in attend_dict:
        attend_dict[name] = [dtString, ""]
    else:
        attend_dict[name][1] = dtString

    with open(attend_csv_path, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["NAME", "ENTRY", "EXIT", "TIME_SPENT_IN_MIN"])
        for name, times in attend_dict.items():
            if name not in ["NAME", "UNKNOWN"]:
                entry_time = times[0]
                exit_time = times[1]
                time_spent = ""
                if entry_time and exit_time:
                    entry_hour, entry_minute = map(int, entry_time[:-3].split(':'))
                    exit_hour, exit_minute = map(int, exit_time[:-3].split(':'))
                    if "PM" in entry_time and "AM" in exit_time:
                        time_spent = ((12 - entry_hour) * 60 + entry_minute) + (exit_hour * 60 + exit_minute)
                    else:
                        time_spent = (exit_hour * 60 + exit_minute) - (entry_hour * 60 + entry_minute)
                writer.writerow([name, entry_time, exit_time, time_spent])

# Function to calculate Eye Aspect Ratio (EAR)
def eye_aspect_ratio(eye):
    A = np.linalg.norm(eye[1] - eye[5])
    B = np.linalg.norm(eye[2] - eye[4])
    C = np.linalg.norm(eye[0] - eye[3])
    ear = (A + B) / (2.0 * C)
    return ear

# Liveness detection function using blink detection
def detect_liveness(shape):
    left_eye = shape[36:42]
    right_eye = shape[42:48]
    left_ear = eye_aspect_ratio(left_eye)
    right_ear = eye_aspect_ratio(right_eye)
    ear = (left_ear + right_ear) / 2.0
    return ear

# Webcam scan for face recognition with enhanced liveness detection
def webcam_scan():
    global encodeListKnown, image_names
    cap = cv2.VideoCapture(0)
    blink_threshold = 0.2
    blink_frames = 3
    consecutive_frames = 0
    blinks = 0
    motion_detected_frames = 0
    frame_diff_threshold = 30
    motion_detection_frames_required = 10
    prev_frame = None

    while True:
        success, img = cap.read()
        if not success:
            break

        imgS = cv2.resize(img, (0, 0), None, 0.25, 0.25)
        imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)

        facesCurFrame = face_recognition.face_locations(imgS)
        encodesCurFrame = face_recognition.face_encodings(imgS, facesCurFrame)

        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        rects = detector(gray, 0)

        if prev_frame is not None:
            frame_diff = cv2.absdiff(prev_frame, gray)
            motion = np.sum(frame_diff > frame_diff_threshold)
            if motion > 1000:  # Adjust threshold as necessary
                motion_detected_frames += 1
            else:
                motion_detected_frames = 0
        prev_frame = gray

        cv2.putText(img, f'Number of faces detected: {len(facesCurFrame)}', (10, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 255), 2)

        for encodeFace, faceLoc, rect in zip(encodesCurFrame, facesCurFrame, rects):
            # Perform liveness detection using blink detection
            shape = predictor(gray, rect)
            shape = np.array([[p.x, p.y] for p in shape.parts()])
            ear = detect_liveness(shape)

            if ear < blink_threshold:
                consecutive_frames += 1
            else:
                if consecutive_frames >= blink_frames:
                    blinks += 1
                consecutive_frames = 0

            if blinks >= 1 and motion_detected_frames >= motion_detection_frames_required:
                # Compare faces with the encodings in the live feed only
                name = "UNKNOWN"
                min_dist = float("inf")
                for known_encode, known_name in zip(encodeListKnown, image_names):
                    face_distance = face_recognition.face_distance([known_encode], encodeFace)
                    if face_distance < min_dist and face_distance < 0.5:  # Adjust threshold as necessary
                        min_dist = face_distance
                        name = known_name.upper()

                print(f"Match found: {name}")
                y1, x2, y2, x1 = faceLoc
                y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4

                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 255, 0), 2)
                cv2.rectangle(img, (x1, y2 - 35), (x2, y2), (255, 255, 0), cv2.FILLED)
                cv2.putText(img, name, (x1 + 6, y2 - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 255), 2)
                save_img(img, name)
                markAttendance(name)

        ret, buffer = cv2.imencode('.jpg', img)
        frame = buffer.tobytes()
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n')

# Routes
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/video_feed')
def video_feed():
    return Response(webcam_scan(), mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/scan')
def scan():
    return redirect(url_for('index'))

@app.route('/attendance')
def attendance():
    with open(attend_csv_path, 'w+', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["NAME", "ENTRY", "EXIT", "TIME_SPENT_IN_MIN"])
        for name, times in attend_dict.items():
            if name not in ["NAME", "UNKNOWN"]:
                entry_time = times[0]
                exit_time = times[1]
                time_spent = ""
                if entry_time and exit_time:
                    entry_hour, entry_minute = map(int, entry_time[:-3].split(':'))
                    exit_hour, exit_minute = map(int, exit_time[:-3].split(':'))
                    if "PM" in entry_time and "AM" in exit_time:
                        time_spent = ((12 - entry_hour) * 60 + entry_minute) + (exit_hour * 60 + exit_minute)
                    else:
                        time_spent = (exit_hour * 60 + exit_minute) - (entry_hour * 60 + entry_minute)
                writer.writerow([name, entry_time, exit_time, time_spent])
    return redirect(url_for('index'))

@app.route('/add_face', methods=['POST'])
def add_face():
    new_name = request.form['name']
    if new_name:
        new_name += ".jpg"
        result, new_img = cv2.VideoCapture(0).read()
        if result:
            cv2.imwrite(os.path.join(path, new_name), new_img)
            images.append(cv2.imread(os.path.join(path, new_name)))
            image_names.append(os.path.splitext(new_name)[0])
            encodeListKnown.append(face_recognition.face_encodings(images[-1])[0])
    return redirect(url_for('index'))

@app.route('/delete_face', methods=['POST'])
def delete_face():
    del_name = request.form['name']
    if del_name in image_names:
        index = image_names.index(del_name)
        del image_names[index]
        del images[index]
        del encodeListKnown[index]
        os.remove(os.path.join(path, f"{del_name}.jpg"))
    return redirect(url_for('index'))

@app.route('/show_images')
def show_images():
    os.startfile(only_name)
    return redirect(url_for('index'))

@app.route('/know_faces')
def know_faces():
    os.startfile(path)
    return redirect(url_for('index'))

@app.route('/about')
def about():
    return render_template('about.html')

if __name__ == "__main__":
    clean()
    access()
    encodeListKnown = find_encodings(images)
    print("Encoding Completed..")
    app.run(debug=True, port=5001)  # Change port if necessary
